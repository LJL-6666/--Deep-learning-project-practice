{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> 机器作诗进阶（2）：LSTM </h1> </center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN存在的问题\n",
    "\n",
    "> * 如果让RNN完成上面的题目，它可以很容易的猜出（1），但是猜不出（2）\n",
    "* 这是因为RNN无法处理“长距离依赖”（Long-Term Dependencies）\n",
    "* RNN的这一短板是由于其算法导致：在训练RNN模型时，每步的梯度随距离会明显减弱，因此当距离过长时，迅速减小的梯度无法将之前的信息传递过去\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进思路\n",
    "> * 原始RNN在隐藏层只有一个状态，即上一时刻的状态，它对于短期的输入非常敏感。\n",
    "* 加入我们再增加一个状态，让它来保存“长期信息”，问题是不是就可以解决啦？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN的进阶：LSTM\n",
    "\n",
    "> * LSTM （Long Short Term Memory Network），也叫长短期记忆网络。\n",
    "* LSTM是RNN的一个优秀的变种模型，能很好的处理“长距离依赖”问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM的改进\n",
    "\n",
    "> 相比于传统的RNN，LSTM主要有两方面的改进：\n",
    "> * 在输入中增加了长期状态（$c_{t-1}$）\n",
    "> * 内部对信息的处理更加复杂：多过程（红色圈圈）+多层网络（黄色方块）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预备知识\n",
    "\n",
    "\n",
    "### $\\tanh$ 变换\n",
    "$\\tanh$ 也是常用的非线性激活函数，可以将一个实数映射到 $(-1,1)$ 的区间，其数学表达式如下：\n",
    "$$\n",
    "\\tanh x = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "与 sigmoid 不同的是，$\\tanh$ 是0均值的,  而且 sigmoid 函数在输入处于0附近时，函数值变化比 $\\tanh$ 敏感，一旦接近或者超出区间就失去敏感性，处于饱和状态\n",
    "\n",
    "### sigmoid 变换\n",
    "\n",
    "sigmoid 函数也叫 Logistic 函数，是神经网络中常用的非线性激活函数，可以将一个实数映射到 $(0,1)$ 的区间，其数学表达式如下：\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM的核心\n",
    "> 使用三个控制开关，控制信息的输入和输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM结构详解\n",
    "\n",
    "### 遗忘门（Forgotten Gate）\n",
    "> * LSTM的第一个环节是进入遗忘门，所谓“遗忘”，是指数据通过这个结构后要“过滤”一部分信息\n",
    "* 这部分的输入为$h_{t-1}$和$x_t$，经过一个sigmoid layer后输出一个$0$和$1$之间的数，越接近于$1$表示保留的信息越多\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM结构详解\n",
    "\n",
    "\n",
    "### 输入门（Input Gate）\n",
    "> * 输入门将决定对下一时刻的状态加入多少“新信息”\n",
    "* 它由两层结构组成：前面的sigmoid layer将决定我们更新数据中的哪几个值，后面的tanh layer则会对数据中所有值输出更新值向量$\\tilde{C_t}$。\n",
    "\n",
    "> 经过上面两步，可以计算当前时刻的长期状态$C_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM结构详解\n",
    "\n",
    "### 输出门（Output Gate）\n",
    "> * 输出门将决定单元的输出值$h_t$。\n",
    "* 它同样由两层结构组成：前面的sigmoid layer将决定我们输出数据中的哪部分值，同时长期状态$C_t$将通过tanh layer转换至$-1$至$1$之间，最后的输出结果为两者的乘积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理：与RNN相同\n",
    "\n",
    "## 数据介绍\n",
    "\n",
    "> `data/poems_clean.txt`\n",
    "* 格式：标题:诗（每句之间以空格隔开）\n",
    "* 例子：\n",
    "    >> 静夜思:床前明月光 疑是地上霜 举头望明月 低头思故乡  \n",
    "    >> 春望:国破山河在 城春草木深 感时花溅泪 恨别鸟惊心 烽火连三月 家书抵万金 白头搔更短 浑欲不胜簪 \n",
    "\n",
    "## 数据的读入与展示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒', '随', '穷', '律', '变', '春', '逐', '鸟', '声', '开', '初', '风', '飘', '带', '柳', '晚', '雪', '间', '花', '梅', '碧', '林', '青', '旧', '竹', '绿', '沼', '翠', '新', '苔', '芝', '田', '初', '雁', '去', '绮', '树', '巧', '莺', '来']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "f = open('data/poems_clean.txt', \"r\", encoding='utf-8')\n",
    "poems = []\n",
    "for line in f.readlines():\n",
    "    title, poem = line.split(':')\n",
    "    poem = poem.replace(' ', '') #将空格去掉\n",
    "    poem = poem.replace('\\n', '') #将换行符去掉\n",
    "    poems.append(list(poem))\n",
    "    \n",
    "print(poems[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整理：文字编码\n",
    "\n",
    "> 读入数据并使用`keras`中的`Tokenizer`为我们的语料库建立词典，给每个字分配一个索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(poems)\n",
    "vocab_size = len(tokenizer.word_index) + 1 #加上停止词0\n",
    "poems_digit = tokenizer.texts_to_sequences(poems)\n",
    "#为了将所有的诗放在一个M*N的np.array中，将每一首诗补0到同样的长度\n",
    "poems_digit = pad_sequences(poems_digit, maxlen=50, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整理：生成$X$和$Y$\n",
    "\n",
    "### （1）数据补全"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始诗歌\n",
      "['床', '前', '明', '月', '光', '疑', '是', '地', '上', '霜', '举', '头', '望', '明', '月', '低', '头', '思', '故', '乡']\n",
      "\n",
      "\n",
      "编码+补全后的结果\n",
      "[532  72  53  13 140 429  44 113  15 202 688 128 106  53  13 502 128  75\n",
      " 134 169   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"原始诗歌\")\n",
    "print(poems[3864])\n",
    "print(\"\\n\")\n",
    "print(\"编码+补全后的结果\")\n",
    "print(poems_digit[3864])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）生成$X$和$Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X示例 \t Y示例\n",
      "42 \t 180\n",
      "180 \t 401\n",
      "401 \t 1143\n",
      "1143 \t 671\n",
      "671 \t 9\n",
      "9 \t 331\n",
      "331 \t 130\n",
      "130 \t 58\n",
      "58 \t 84\n",
      "84 \t 177\n",
      "... \t ...\n"
     ]
    }
   ],
   "source": [
    "X = poems_digit[:, :-1]\n",
    "Y = poems_digit[:, 1:]\n",
    "print(\"X示例\", \"\\t\", \"Y示例\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(X[0][i], \"\\t\", Y[0][i])\n",
    "    \n",
    "print(\"...\", \"\\t\", \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）把Y变成One-Hot向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24117, 49, 5546)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y, num_classes=vocab_size)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建LSTM的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 49, 128)           709888    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 49, 64)            49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 49, 5546)          360490    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 49, 5546)          0         \n",
      "=================================================================\n",
      "Total params: 1,119,786\n",
      "Trainable params: 1,119,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, BatchNormalization\n",
    "from keras import Model\n",
    "\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "\n",
    "inp = Input(shape=(49,))\n",
    "\n",
    "# Encoder\n",
    "x = Embedding(vocab_size, hidden_size1, input_length=49, mask_zero=True)(inp)\n",
    "x = LSTM(hidden_size2, return_sequences=True)(x)\n",
    "\n",
    "# prediction\n",
    "x = Dense(vocab_size)(x)\n",
    "pred = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inp, pred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 让我们来数数参数\n",
    "\n",
    "### Embedding层\n",
    "我们一共有5546个字，每个字嵌入到一个128维的空间中，所以参数个数为：$5546\\times 128=70988$。 \n",
    "\n",
    "### LSTM层\n",
    "第一、参考之前LSTM的介绍可以知道，需要参数估计的非线性变幻主要涉及到：$f_t$, $i_t$, $\\tilde C_t$, 还有 $o_t$。每个非线性变化所消耗的参数一样。背后主要的原因是：TF要求$h_t$和$c_t$的维度一样（理论上完全可以不一样）。\n",
    "\n",
    "第二、以$f_t$为例，它作用在$(h_{t-1},x_{t})$上面。由于$h_{t-1}$和$x_{t}$分别是一个64维和128维的向量。加上截距项，需要64+128+1=193个参数。这些参数应用到遗忘门，帮助$C_t$状态更新的时候，需要消耗：193*64=12352个参数。\n",
    "\n",
    "第三，因为，$f_t$, $i_t$, $\\tilde C_t$, 还有$o_t$一共4个非线性变换，而每一个变换所消耗的参数都是12352，因此，最终所需的所有参数是：$12352\\times 4=49408$。\n",
    "\n",
    "### dense层\n",
    "这是一个5546类的分类问题，输入就是h维度+常数项，所以参数个数为：$(64+1)\\times 5546=360490$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用模型作诗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熊颀颀颀坑\n",
      "大人不不不\n",
      "很不不不不\n",
      "帅不不不不\n"
     ]
    }
   ],
   "source": [
    "poem_incomplete = '熊****大****很****帅****'\n",
    "poem_index = []\n",
    "poem_text = ''\n",
    "for i in range(len(poem_incomplete)):\n",
    "    current_word = poem_incomplete[i]\n",
    "    \n",
    "    if  current_word != '*':\n",
    "        index = tokenizer.word_index[current_word]\n",
    "        \n",
    "    else:\n",
    "        x = np.expand_dims(poem_index, axis=0)\n",
    "        x = pad_sequences(x, maxlen=49, padding='post')\n",
    "        y = model.predict(x)[0, i]\n",
    "        \n",
    "        y[0] = 0            #去掉停止词\n",
    "        index = y.argmax()\n",
    "        current_word = tokenizer.index_word[index]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    poem_index.append(index)\n",
    "    poem_text = poem_text + current_word\n",
    "        \n",
    "poem_text = poem_text[0:]\n",
    "print(poem_text[0:5])\n",
    "print(poem_text[5:10])\n",
    "print(poem_text[10:15])\n",
    "print(poem_text[15:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 思考问题：RNN vs. 时间序列模型"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
